{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"#113D68\" size=5>X. Deep Learning para Procesamiento del Lenguaje Natural</font></h1>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font color=\"#113D68\" size=6>3. Proyecto - Desarrollar un Modelo CNN n-gramas para análisis de reseñas</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "* [0. Contexto](#section0)\n",
    "* [1. Conjunto de datos de reseñas de películas](#section1)\n",
    "* [2. Preparación de datos](#section2)\n",
    "    * [2.1. Dividir en conjuntos de train y test](#section21)\n",
    "    * [2.2. Revisiones de carga y limpieza](#section22)\n",
    "    * [2.3. Limpiar todas las reseñas y guardar](#section23)\n",
    "* [3. Desarrollar modelo multicanal](#section3)\n",
    "    * [3.1. Codificar datos](#section31)\n",
    "    * [3.2. Definir el modelo](#section32)\n",
    "    * [3.3. Mostrar los resultados](#section33)\n",
    "* [4. Evaluar modelo](#section4)\n",
    "* [5. Extensiones](#section5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 0. Contexto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de aprendizaje profundo estándar para la clasificación de texto y el análisis de sentimientos utiliza una capa de incrustación de palabras y una red neuronal convolucional unidimensional. El modelo se puede expandir usando múltiples redes neuronales convolucionales paralelas que leen el documento fuente usando diferentes tamaños de kernel. Esto, en efecto, crea una red neuronal convolucional multicanal para texto que lee texto con diferentes tamaños de n-gramas (grupos de palabras). \n",
    "\n",
    "En este tutorial, descubrirá cómo desarrollar una red neuronal convolucional multicanal para la predicción de sentimientos en los datos de revisión de películas de texto. Después de completar este tutorial, sabrás:\n",
    "- Cómo preparar datos de texto de reseñas de películas para el modelado.\n",
    "- Cómo desarrollar una red neuronal convolucional multicanal para texto en Keras.\n",
    "- Cómo evaluar un modelo de ajuste en datos no etiquetados de reseñas de películas.\n",
    "\n",
    "Este tutorial se divide en las siguientes partes:\n",
    "1. Conjunto de datos de revisión de películas.\n",
    "2. Preparación de datos.\n",
    "3. Desarrollar modelo multicanal.\n",
    "4. Evaluar modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\" size=6>1. Conjunto de datos de reseñas de películas</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial, utilizaremos el conjunto de datos de revisión de películas. Este conjunto de datos diseñado para el análisis de sentimientos se describió anteriormente en el Capítulo 9. \n",
    "\n",
    "Después de descomprimir el archivo, tendrá un directorio llamado `txt_sentoken` con dos subdirectorios que contienen el texto `neg` y `pos` para reseñas negativas y positivas. Las revisiones se almacenan una por archivo con una convención de nomenclatura `cv000` a `cv999` para cada `neg` y `pos`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Desacargar el dataset [Review polarity](https://raw.githubusercontent.com/jbrownlee/Datasets/master/review_polarity.tar.gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6>2. Preparación de datos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: La preparación del conjunto de datos de reseñas de películas se describió por primera vez en capítulos anteriores. En esta sección, veremos 3 cosas:\n",
    "1. Separación de datos en conjuntos de train y test.\n",
    "2. Cargar y limpiar los datos para eliminar puntuación y números.\n",
    "3. Limpiar todas las reseñas y guardarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section21\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.1. Dividir en conjuntos de train y test</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretendemos que estamos desarrollando un sistema que puede predecir el sentimiento de una reseña textual de una película como positivo o negativo. Esto significa que después de que se desarrolle el modelo, necesitaremos hacer predicciones sobre nuevas revisiones textuales. Esto requerirá que se realice la misma preparación de datos en esas revisiones nuevas que se realiza en los datos de entrenamiento para el modelo. Nos aseguraremos de que esta restricción se incorpore en la evaluación de nuestros modelos al dividir los conjuntos de datos de entrenamiento y prueba antes de cualquier preparación de datos. Esto significa que cualquier conocimiento sobre los datos en el conjunto de prueba que podría ayudarnos a preparar mejor los datos (por ejemplo, las palabras utilizadas) no está disponible en la preparación de los datos utilizados para entrenar el modelo.\n",
    "\n",
    "Dicho esto, utilizaremos las últimas 100 reseñas positivas y las últimas 100 reseñas negativas como conjunto de prueba (100 reseñas) y las 1800 reseñas restantes como conjunto de datos de entrenamiento. Este es un tren del 90%, división del 10% de los datos. La división se puede imponer fácilmente mediante el uso de los nombres de archivo de las revisiones donde las revisiones denominadas 000 a 899 son para datos de entrenamiento y las revisiones denominadas 900 en adelante son para prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section22\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.2. Revisiones de carga y limpieza</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de texto ya están bastante limpios; no se requiere mucha preparación. Sin atascarnos demasiado en los detalles, prepararemos los datos de la siguiente manera:\n",
    "- Fichas divididas en espacios en blanco.\n",
    "- Eliminar toda la puntuación de las palabras.\n",
    "- Eliminar todas las palabras que no estén compuestas únicamente por caracteres alfabéticos.\n",
    "- Eliminar todas las palabras que son palabras vacías conocidas.\n",
    "- Eliminar todas las palabras que tengan una longitud $\\leq$ 1 carácter.\n",
    "\n",
    "Podemos poner todos estos pasos en una función llamada `clean_doc()` que toma como argumento el texto sin formato cargado desde un archivo y devuelve una lista de tokens limpios. También podemos definir una función `load_doc()` que cargue un documento desde un archivo listo para usar con la función `clean_doc()`. A continuación se muestra un ejemplo de limpieza de la primera crítica positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load the document\n",
    "filename = 'data/txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar el ejemplo imprime una larga lista de tokens limpios. Hay muchos más pasos de limpieza que queramos explorar y los dejo como ejercicios adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section23\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.3. Limpiar todas las reseñas y guardar</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usar la función para limpiar reseñas y aplicarla a todas las reseñas. Para hacer esto, desarrollaremos una nueva función llamada `process_docs()` a continuación que recorrerá todas las revisiones en un directorio, las limpiará y las devolverá como una lista. También agregaremos un argumento a la función para indicar si la función está procesando revisiones de entrenamiento o prueba, de esa manera los nombres de archivo se pueden filtrar (como se describió anteriormente) y solo se limpiarán y devolverán las revisiones de entrenamiento o prueba solicitadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, is_train):\n",
    "\tdocuments = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif is_train and filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\tif not is_train and not filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load the doc\n",
    "\t\tdoc = load_doc(path)\n",
    "\t\t# clean doc\n",
    "\t\ttokens = clean_doc(doc)\n",
    "\t\t# add to list\n",
    "\t\tdocuments.append(tokens)\n",
    "\treturn documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos llamar a esta función con revisiones de entrenamiento negativas. También necesitamos etiquetas para el train y test. Sabemos que tenemos 900 documentos de capacitación y 100 documentos de prueba. Podemos usar una lista de comprensión de Python para crear las etiquetas para las revisiones negativas _(0)_ y positivas _(1)_ para los conjuntos de entrenamiento y prueba. La siguiente función denominada `load_clean_dataset()` cargará y limpiará el texto de la reseña de la película y también creará las etiquetas para las reseñas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean a dataset\n",
    "def load_clean_dataset(is_train):\n",
    "\t# load documents\n",
    "\tneg = process_docs('data/txt_sentoken/neg', is_train)\n",
    "\tpos = process_docs('data/txt_sentoken/pos', is_train)\n",
    "\tdocs = neg + pos\n",
    "\t# prepare labels\n",
    "\tlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "\treturn docs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, queremos guardar los conjuntos train y test preparados en un archivo para que podamos cargarlos más tarde para el modelado y la evaluación del modelo. La función debajo llamada `save_dataset()` guardará un conjunto de datos preparado dado (elementos `X` e `y`) en un archivo usando la API `pickle` (esta es la API estándar para guardar objetos en Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a dataset to file\n",
    "def save_dataset(dataset, filename):\n",
    "\tdump(dataset, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos llamar a las funciones para ejecutar nuestro ejemplo y que queden los corpus limpios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train.pkl\n",
      "Saved: test.pkl\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from pickle import dump\n",
    "\n",
    "# load and clean all reviews\n",
    "train_docs, ytrain = load_clean_dataset(True)\n",
    "test_docs, ytest = load_clean_dataset(False)\n",
    "# save training datasets\n",
    "save_dataset([train_docs, ytrain], 'train.pkl')\n",
    "save_dataset([test_docs, ytest], 'test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el ejemplo, se limpian los documentos de revisión de películas de texto, se crean etiquetas y se guardan los datos preparados para los conjuntos de datos de entrenamiento y prueba en train.pkl y test.pkl respectivamente. Ahora estamos listos para desarrollar nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6>3. Desarrollar modelo multicanal</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, desarrollaremos una red neuronal convolucional multicanal para el problema de predicción del análisis de reseñas. Esta sección se divide en 3 partes:\n",
    "1. Codificar datos.\n",
    "2. Definir modelo.\n",
    "3. Mostrar los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section31\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.1. Codificar datos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es cargar el conjunto de datos de entrenamiento limpio. Se puede llamar a la función denominada `load_dataset()` a continuación para cargar el conjunto de datos de entrenamiento en pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, debemos colocar un `Tokenizador` de Keras en el conjunto de datos de entrenamiento. Usaremos este tokenizador para definir el vocabulario de la capa `Embedding` y codificar los documentos de revisión como números enteros. La función `create_tokenizer()` a continuación creará un `Tokenizer` dada una lista de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También necesitamos saber la longitud máxima de las secuencias de entrada como entrada para el modelo y rellenar todas las secuencias a la longitud fija. La función `max_length()` calculará la longitud máxima (número de palabras) para todas las revisiones en el conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    #return max([len(s.split()) for s in lines])\n",
    "    return max([len(s) for s in lines])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También necesitamos saber el tamaño del vocabulario para la capa `Embedding`. Esto se puede calcular a partir del `Tokenizer` preparado, de la siguiente manera:\n",
    "```python\n",
    "    # calculate vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos codificar enteros y rellenar el texto limpio de la reseña de la película. La función de abajo\n",
    "`encode_text()` codificará y rellenará los datos de texto a la longitud máxima de revisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "\treturn padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section32\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.2. Definir el modelo</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo estándar para la clasificación de documentos es usar una capa `Embedding` como entrada, seguida de una red neuronal convolucional unidimensional, una capa de agrupación y luego una capa de salida de predicción. El tamaño del kernel en la capa convolucional define el número de palabras a considerar como la convolución se pasa a través del documento de texto de entrada, proporcionando un parámetro de agrupación. Una red neuronal convolucional multicanal para la clasificación de documentos implica el uso de múltiples versiones del modelo estándar con kernels de diferentes tamaños. Esto permite que el documento se procese a diferentes resoluciones o diferentes n-gramas (grupos de palabras) a la vez, mientras que el modelo aprende cómo integrar mejor estas interpretaciones.\n",
    "\n",
    "Este enfoque fue descrito por primera vez por Yoon Kim en su artículo de 2014 titulado _Convolutional Neural Networks for Sentence Classification_. En el documento, Kim experimentó con capas incrustadas estáticas y dinámicas (actualizadas), podemos simplificar el enfoque y, en su lugar, centrarnos solo en el uso de diferentes tamaños de kernel. Este enfoque se comprende mejor con un diagrama tomado del artículo de Kim, consulte el Capítulo\n",
    "anteriores.\n",
    "\n",
    "En Keras, se puede definir un modelo de entrada múltiple utilizando la API funcional. Definiremos un modelo con tres canales de entrada para procesar 4-gramas, 6-gramas y 8-gramas de texto de reseñas de películas. Cada canal se compone de los siguientes elementos:\n",
    "- Capa `Input` que define la longitud de las secuencias de entrada.\n",
    "- Capa `Embedding` ajustada al tamaño del vocabulario y representaciones de valor real de 100 dimensiones.\n",
    "- Capa `Conv1D` con 32 filtros y un tamaño de kernel establecido en la cantidad de palabras para leer a la vez.\n",
    "- Capa `MaxPooling1D` para consolidar la salida de la capa convolucional.\n",
    "- Capa `Flatten` para reducir la salida tridimensional a bidimensional para la concatenación.\n",
    "La salida de los tres canales se concatena en un solo vector y se procesa mediante una capa `Dense` y una capa de\n",
    "salida. La siguiente función define y devuelve el modelo. Como parte de la definición del modelo, se imprime un resumen del modelo definido y se crea un gráfico del modelo y se guarda en un archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "\tconv1 = Conv1D(32, 4, activation='relu')(embedding1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D()(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "\t# channel 2\n",
    "\tinputs2 = Input(shape=(length,))\n",
    "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "\tconv2 = Conv1D(32, 6, activation='relu')(embedding2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D()(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "\t# channel 3\n",
    "\tinputs3 = Input(shape=(length,))\n",
    "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "\tconv3 = Conv1D(32, 8, activation='relu')(embedding3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D()(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "\t# merge\n",
    "\tmerged = concatenate([flat1, flat2, flat3])\n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tmodel.summary()\n",
    "\tplot_model(model, show_shapes=True, to_file='model.png')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se guarda un gráfico del modelo definido en un archivo con el nombre `model.png`.\n",
    "  <img src=\"img/model_53.png\" width=\"500\" height=\"500\" alt=\"CNN NLP\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section33\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.3. Mostrar los resultados</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos ver el impacto de nuestra red neurnoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 1380\n",
      "Vocabulary size: 44277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 10:57:48.555812: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-09-13 10:57:48.555964: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (manwest-PC): /proc/driver/nvidia/version does not exist\n",
      "2022-09-13 10:57:48.557748: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1380)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1380)]       0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 1380)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1380, 100)    4427700     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1380, 100)    4427700     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1380, 100)    4427700     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 1377, 32)     12832       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 1375, 32)     19232       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 1373, 32)     25632       ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1377, 32)     0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1375, 32)     0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 1373, 32)     0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 688, 32)      0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 687, 32)     0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 686, 32)     0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 22016)        0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 21984)        0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 21952)        0           ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 65952)        0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           659530      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            11          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,000,337\n",
      "Trainable params: 14,000,337\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/7\n",
      "113/113 [==============================] - 87s 708ms/step - loss: 0.6899 - accuracy: 0.5350\n",
      "Epoch 2/7\n",
      "113/113 [==============================] - 72s 635ms/step - loss: 0.4541 - accuracy: 0.7833\n",
      "Epoch 3/7\n",
      "113/113 [==============================] - 74s 656ms/step - loss: 0.0503 - accuracy: 0.9856\n",
      "Epoch 4/7\n",
      "113/113 [==============================] - 73s 646ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 5/7\n",
      "113/113 [==============================] - 67s 596ms/step - loss: 4.6277e-04 - accuracy: 1.0000\n",
      "Epoch 6/7\n",
      "113/113 [==============================] - 66s 586ms/step - loss: 2.2355e-04 - accuracy: 1.0000\n",
      "Epoch 7/7\n",
      "113/113 [==============================] - 68s 599ms/step - loss: 1.9357e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import concatenate\n",
    "\n",
    "# load training dataset\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length    \n",
    "length = max_length(trainLines)\n",
    "print('Max document length: %d' % length)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], array(trainLabels), epochs=7, batch_size=16)\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar el ejemplo primero imprime un resumen del conjunto de datos de entrenamiento preparado.\n",
    "\n",
    "El modelo se ajusta relativamente rápido y parece mostrar una buena habilidad en el conjunto de datos de entrenamiento.\n",
    "\n",
    "Se guarda un gráfico del modelo definido en un archivo, que muestra claramente los tres canales de entrada para el modelo.\n",
    "\n",
    "El modelo se ajusta a varias épocas y se guarda en el archivo `model.h5` para una evaluación posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "# <font color=\"#004D7F\" size=6>4. Evaluar modelo</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, podemos evaluar el modelo de ajuste al predecir la reseña en todas las revisiones en el conjunto de datos de prueba no etiquetados. Usando las funciones de carga de datos desarrolladas en la sección anterior, podemos cargar y codificar los conjuntos de datos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 1380\n",
      "Vocabulary size: 44277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 12:21:51.113448: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-09-13 12:21:51.113573: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (manwest-PC): /proc/driver/nvidia/version does not exist\n",
      "2022-09-13 12:21:51.114868: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load datasets\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "testLines, testLabels = load_dataset('test.pkl')\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "print('Max document length: %d' % length)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "# load the model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargado el modelo guardado podemos evaluarlo tanto en los conjuntos de datos de entrenamiento como de prueba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00\n",
      "Test Accuracy: 87.50\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "_, acc = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)\n",
    "print('Train Accuracy: %.2f' % (acc*100))\n",
    "# evaluate model on test dataset dataset\n",
    "_, acc = model.evaluate([testX,testX,testX], array(testLabels), verbose=0)\n",
    "print('Test Accuracy: %.2f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar el ejemplo imprime la habilidad del modelo en los conjuntos de datos de entrenamiento y prueba. Podemos ver que, como era de esperar, la habilidad en el conjunto de datos de entrenamiento es excelente, aquí con una precisión del 100%.\n",
    "\n",
    "También podemos ver que la habilidad del modelo en el conjunto de datos de prueba no etiquetados también es muy impresionante, logrando un 88,5%, que está por encima de la habilidad del modelo informado en el documento de 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "# <font color=\"#004D7F\" size=6>5. Extensiones</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se enumeran algunas ideas para ampliar el tutorial que tal vez desee explorar.\n",
    "- __Diferentes n-gramas__. Explore el modelo cambiando el tamaño del kernel (número de n-gramas) que utilizan los canales en el modelo para ver cómo afecta la habilidad del modelo.\n",
    "- __Más o menos canales__. Explore el uso de más o menos canales en el modelo y vea cómo afecta la habilidad del modelo.\n",
    "- __Incrustación compartida__. Explore configuraciones donde cada canal comparte la misma palabra incrustada e informe sobre el impacto en la habilidad del modelo.\n",
    "- __Red más profunda__. Las redes neuronales convolucionales funcionan mejor en visión artificial cuando son más profundas. Explore el uso de modelos más profundos aquí y vea cómo afecta la habilidad del modelo.\n",
    "- __Secuencias truncadas__. Rellenar todas las secuencias con la duración de la secuencia más larga puede ser extremo si la secuencia más larga es muy diferente a todas las demás revisiones. Estudie la distribución de la duración de las reseñas y trunque las reseñas a una duración media.\n",
    "- __Vocabulario truncado__. Eliminamos las palabras que aparecían con poca frecuencia, pero aún teníamos un gran vocabulario de más de 25,000 palabras. Explore aún más la reducción del tamaño del vocabulario y el efecto en la habilidad del modelo.\n",
    "- __Épocas y tamaño de lote__. El modelo parece ajustarse rápidamente al conjunto de datos de entrenamiento. Explore configuraciones alternativas de la cantidad de épocas de entrenamiento y el tamaño del lote y use el conjunto de datos de prueba como un conjunto de validación para elegir un mejor punto de parada para entrenar el modelo.\n",
    "- __Pre-entrenamiento e incrustación__. Explore el entrenamiento previo de una palabra Word2Vec incrustada en el modelo y el impacto en la habilidad del modelo con y sin ajustes adicionales durante el entrenamiento.\n",
    "- __Utilice la incrustación de guantes__. Explore la carga de la incrustación de GloVe previamente entrenada y el impacto en la habilidad del modelo con y sin más ajustes durante el entrenamiento.\n",
    "- __Entrenamiento del Modelo Final__. Entrene un modelo final con todos los datos disponibles y utilícelo para hacer predicciones sobre reseñas de películas ad hoc reales de Internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
