{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"#113D68\" size=6>Deep Learning con Python y Keras</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=5>Parte 6. Redes Neuronales Recurrentes</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=4>4. LSTM para Clasificación</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "* [0. Contexto](#section0)\n",
    "* [1. LSTM para clasificación](#section1)\n",
    "    * [1.1. Librerías](#section1.1)\n",
    "    * [1.2. Dataset](#section1.2)\n",
    "    * [1.3. Truncar las secuencias de entrada](#section1.3)\n",
    "    * [1.4. Modelo de linea base](#section1.4)\n",
    "    * [1.5. Resultados](#section1.5)\n",
    "* [2. LSTM con Dropout](#section2)\n",
    "* [3. LSTM con Dropout recurrente](#section3)\n",
    "* [4. LSTM y CNN para clasificación](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 0. Contexto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clasificación de secuencias es un problema de modelado predictivo en el que tiene alguna secuencia de entradas en el espacio o el tiempo y la tarea es predecir una categoría para la secuencia. Lo que dificulta este problema es que las secuencias pueden variar en longitud, estar compuestas por un vocabulario muy amplio de símbolos de entrada y pueden requerir que el modelo aprenda el contexto a largo plazo o las dependencias entre símbolos en la secuencia de entrada. En este proyecto, descubrirá cómo puede desarrollar modelos de redes neuronales recurrentes LSTM para problemas de clasificación de secuencias en Python utilizando la biblioteca de aprendizaje profundo de Keras. Después de completar este proyecto, sabrá:\n",
    "\n",
    "* Cómo desarrollar un modelo LSTM para un problema de clasificación de secuencias.\n",
    "* Cómo reducir el sobreajuste en sus modelos LSTM mediante el uso de abandonos.\n",
    "* Cómo combinar modelos LSTM con redes neuronales convolucionales que sobresalen en el aprendizaje de relaciones espaciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\" size=6>1. LSTM para clasificación</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema que usaremos para demostrar el aprendizaje de secuencias en este tutorial es el problema de clasificación de sentimientos de revisión de películas de IMDB. Podemos desarrollar rápidamente un pequeño LSTM para el problema de IMDB y lograr una buena precisión. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Puede obtener más información sobre el dataset [IMBD](http://ai.stanford.edu/~amaas/data/sentiment/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1.1\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.1. Funciones</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos importando las clases y funciones necesarias para este modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1.2\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.2. Dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos cargar el conjunto de datos de IMDB. Limitamos el conjunto de datos a las 5.000 palabras principales. También dividimos el conjunto de datos en conjuntos de entranmiento (50%) y validación (50%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1.3\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.3. Truncar las secuencias de entrada</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, necesitamos truncar y rellenar las secuencias de entrada para que todas tengan la misma longitud para el modelado. El modelo aprenderá que los valores cero no contienen información, de modo que las secuencias no tienen la misma longitud en términos de contenido, pero se requieren vectores de la misma longitud para realizar el cálculo en Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1.4\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.4. Modelo de linea base</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos definir, compilar y ajustar nuestro modelo LSTM. La primera capa es la capa incrustada que utiliza 32 vectores de longitud para representar cada palabra. La siguiente capa es la capa LSTM con 100 unidades de memoria (neuronas inteligentes). \n",
    "\n",
    "Finalmente, debido a que este es un problema de clasificación, usamos una capa de salida densa con una sola neurona y una función de activación sigmoidea para hacer predicciones 0 o 1 para las dos clases (buenas y malas) en el problema. Debido a que es un problema de clasificación binaria, la pérdida logarítmica se utiliza como función de pérdida (entropía cruzada binaria en Keras). \n",
    "\n",
    "Se utiliza el eficiente algoritmo de optimización de ADAM. El modelo es apto para solo 3 épocas porque rápidamente se adapta al problema. Se utiliza un gran tamaño de lote de 64 revisiones para espaciar las actualizaciones de peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 119s 305ms/step - loss: 0.4594 - accuracy: 0.7775\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 122s 312ms/step - loss: 0.2992 - accuracy: 0.8828\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 105s 269ms/step - loss: 0.2568 - accuracy: 0.8993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f20e0485e48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1.5\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.5. Resultados</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ajustado, estimamos el rendimiento del modelo en revisiones no vistas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.85%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ver que este sencillo LSTM con pocos ajustes logra resultados casi de vanguardia en el problema de IMDB. Es importante destacar que esta es una plantilla que puede utilizar para aplicar redes LSTM a sus propios problemas de clasificación de secuencia. \n",
    "\n",
    "Ahora, echemos un vistazo a algunas extensiones de este modelo simple que quizás también desee incorporar a sus propios problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6>2. LSTM con Dropout</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales recurrentes como LSTM generalmente tienen el problema de sobreajuste. Dropout se puede aplicar entre capas utilizando la capa Dropout. Podemos hacer esto fácilmente agregando nuevas capas Dropout entre las capas Embedding y LSTM y las capas de salida LSTM y Dense. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 97s 247ms/step - loss: 0.5842 - accuracy: 0.7084\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 112s 286ms/step - loss: 0.3316 - accuracy: 0.8629\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 122s 312ms/step - loss: 0.3582 - accuracy: 0.8429\n",
      "Accuracy: 84.45%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que Dropout tiene el impacto deseado en el entrenamiento con una tendencia de convergencia ligeramente más lenta y, en este caso, una precisión final menor. \n",
    "\n",
    "El modelo probablemente podría usar algunas épocas más de entrenamiento y puede lograr una habilidad más alta (pruébelo y vea). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6>3. LSTM con Dropout recurrente</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente, Dropout se puede aplicar a las conexiones de entrada y recurrentes de las unidades de memoria con el LSTM de manera precisa y por separado. Keras proporciona esta capacidad con parámetros en la capa LSTM, Dropout para configurar el Dropout de entrada y `recurrent_dropout` para configurar el Dropout recurrente. Por ejemplo, podemos modificar el primer ejemplo para agregar Dropout a la entrada y conexiones recurrentes de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que Dropout recurrente de LSTM tiene un efecto más pronunciado en la convergencia de la red que la Dropout por capas. \n",
    "\n",
    "Como se indicó anteriormente, el número de épocas se mantuvo constante y podría aumentarse para ver si la habilidad del modelo se puede mejorar aún más. \n",
    "\n",
    "Dropout es una técnica poderosa para combatir el sobreajuste en sus modelos LSTM y es una buena idea probar ambos métodos, pero puede apostar mejores resultados con la deserción específica de la puerta proporcionada en Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "# <font color=\"#004D7F\" size=6>4. LSTM y CNN para clasificación</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales convolucionales sobresalen en el aprendizaje de la estructura espacial en los datos de entrada. Los datos de revisión de IMDB tienen una estructura espacial unidimensional en la secuencia de palabras en las revisiones y la CNN puede seleccionar características invariantes para el sentimiento bueno y malo. Estas características espaciales aprendidas pueden luego ser aprendidas como secuencias por una capa LSTM. Podemos agregar fácilmente una CNN unidimensional y capas de agrupación máxima después de la capa de incrustación que luego alimentan las características consolidadas al LSTM.\n",
    "\n",
    "Podemos utilizar un conjunto más pequeño de 32 entidades con una longitud de filtro pequeña de 3. La capa de agrupación puede utilizar la longitud estándar de 2 para reducir a la mitad el tamaño del mapa de entidades. Por ejemplo, crearíamos el modelo de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que logramos resultados similares al primer ejemplo aunque con menos pesos y un tiempo de entrenamiento más rápido. Esperaría que se pudieran lograr resultados aún mejores si este ejemplo se ampliara aún más para usar el abandono."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
