{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"#113D68\" size=5>Deep Learning para Procesamiento del Lenguaje Natural</font></h1>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font color=\"#113D68\" size=6>Cómo Limpiar Texto Manualmente y con NLTK</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "* [0. Contexto](#section0)\n",
    "* [1. Tokenización y limpieza con NLTK](#section1)\n",
    "    * [1.1. Metamorfosis de Franz Kafka](#section11)\n",
    "    * [1.2. Instalación NLTK](#section12)\n",
    "    * [1.3. Dividir en oraciones](#section13)\n",
    "    * [1.4. Dividir en palabras](#section14)\n",
    "    * [1.5. Filtrar la puntuación](#section15)\n",
    "    * [1.6. Filtrar palabras vacías (y pipeline)](#section16)\n",
    "    * [1.7. Palabras raíz](#section17)\n",
    "* [2. Análisis de datos](#section2)\n",
    "    * [2.1. Conteo de palabras con CountVectorizer](#section21)\n",
    "    * [2.2. Frecuencias de palabras con TfidfVectorizer](#section22)\n",
    "    * [2.3. Hashing con HashingVectorizer](#section23)\n",
    "* [3. Preparación de texto](#section3)\n",
    "    * [3.1. Dividir palabras con text_to_word_sequence](#section31)\n",
    "    * [3.2. Codificación con one_hot](#section32)\n",
    "    * [3.3. Codificación hash con hashing_trick](#section33)\n",
    "    * [3.4. API de Tokenizador](#section34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\" size=6>1. Tokenización y limpieza con NLTK</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El kit de herramientas de lenguaje natural (NLTK), es una biblioteca de Python escrita para trabajar y modelar texto. Proporciona buenas herramientas para cargar y limpiar texto que podemos usar para preparar nuestros datos para trabajar con algoritmos de aprendizaje automático y aprendizaje profundo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section11\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.1. Metamorfosis de Franz Kafka</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos seleccionando un conjunto de datos. En este tutorial, utilizaremos el texto del libro _Metamorphosis_ de Franz Kafka. No hay razón específica, aparte de que es corto, me gusta, y puede que también te guste. Espero que sea uno de esos clásicos que la mayoría de los estudiantes tienen que leer en la escuela. El texto completo de _Metamorphosis_ está disponible de forma gratuita en Project Gutenberg.\n",
    "\n",
    "El Proyecto Gutenberg agrega un encabezado y un pie de página estándar a cada libro y esto no es parte del texto original. Abra el archivo en un editor de texto y elimine el encabezado y el pie de página. El encabezado es obvio y termina con el texto:\n",
    "```\n",
    "        *** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\n",
    "```\n",
    "\n",
    "El pie de página es todo el texto después de la línea de texto que dice:\n",
    "```\n",
    "        *** END OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre el [Proyecto Gutenberg](https://www.gutenberg.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Descargar el libro [Metamorphosis de Franz Kafka Plain Text UTF-8](http://www.gutenberg.org/cache/epub/5200/pg5200.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Descargue el archivo y colóquelo en su directorio de trabajo actual con el nombre de archivo `metamorphosis.txt`. \n",
    "2. El archivo contiene información de encabezado y pie de página que no nos interesa, específicamente información de derechos de autor y licencia. \n",
    "3. Abra el archivo y elimine la información del encabezado y pie de página y guarde el archivo como `metamorphosis_clean.txt`. \n",
    "4. El inicio del archivo limpio debería verse así:\n",
    "    - _One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin._\n",
    "5. El archivo debe terminar con:\n",
    "    - _And, as if in confirmation of their new dreams and good intentions, as soon as they reached their destination Grete was the first to get up and stretch out her young body._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section12\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.2. Instalación NLTK</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede instalar NLTK usando su administrador de paquetes favorito, como `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de la instalación, deberá instalar los datos utilizados con la biblioteca, incluido un gran conjunto de documentos que puede usar más tarde para probar otras herramientas en NLTK. Hay algunas formas de hacer esto, como desde dentro de un script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "Hit Enter to continue: \n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "Hit Enter to continue: \n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "Hit Enter to continue: \n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "Hit Enter to continue: \n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet2021......... Open English Wordnet 2021\n",
      "  [ ] wordnet31........... Wordnet 3.1\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "Hit Enter to continue: \n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O desde la línea de comando que es más cómodo\n",
    "```python\n",
    "python -m nltk.downloader all\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section13\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.3. Dividir en oraciones</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un buen primer paso útil es dividir el texto en oraciones. Algunas tareas de modelado prefieren que la entrada sea en forma de párrafos u oraciones, como `Word2Vec`. \n",
    "1. Primero puede dividir el texto en oraciones, \n",
    "2. dividir cada oración en palabras y \n",
    "3. luego guardar cada oración en un archivo, una por línea.\n",
    "\n",
    "NLTK proporciona la función `tokenize()` enviada para dividir el texto en oraciones. El siguiente ejemplo carga el archivo `metamorphosis_clean.txt` en la memoria, lo divide en oraciones e imprime la primera oración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poder utilizar la función `sent_tokenize` debe de estar previametne descargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/manwest/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "# load data\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el ejemplo, podemos ver que aunque el documento se divide en oraciones, cada oración aún conserva la nueva línea del envoltorio artificial de las líneas en el documento original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section14\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.4. Dividir en palabras</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK proporciona una función llamada `word_tokenize()` para dividir cadenas en tokens (nominalmente palabras). Divide tokens en función de los espacios en blanco y la puntuación. Por ejemplo, las comas y los puntos se toman como tokens separados. Las contracciones se separan (por ejemplo, _What's_ se convierte en _What_ y _'s_).\n",
    "Las puntuaciones, comas y - se mantienen, y así sucesivamente. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '``', 'What', \"'s\", 'happened', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# load data\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el código, podemos ver que la puntuación ahora son tokens que luego podríamos decidir filtrar específicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section15\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.5. Filtrar la puntuación</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos filtrar todos los tokens que no nos interesen, como todos los signos de puntuación independientes. Esto se puede hacer iterando sobre todos los tokens y manteniendo solo aquellos tokens que son todos alfabéticos. Python tiene la función `isalpha()` que se puede usar. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 'happened', 'to', 'me', 'he', 'thought', 'It', 'was', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# load data\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el ejemplo, puede ver que no solo los tokens de puntuación, sino también ejemplos como _armour-like_ y _'s_ se filtraron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section16\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.6. Filtrar palabras vacías (y pipeline)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras vacías son aquellas palabras que no contribuyen al significado más profundo de la frase. Son las palabras más comunes como: _the_, _a_, and _is_. Para algunas aplicaciones, como la clasificación de documentación, puede tener sentido eliminar las palabras vacías. NLTK proporciona una lista de palabras vacías comúnmente acordadas para una variedad de idiomas, como el inglés. Se pueden cargar de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ver que están todos en minúsculas y se les ha quitado la puntuación. Puede comparar sus tokens con las palabras vacías y filtrarlas, pero debe asegurarse de que su texto esté preparado de la misma manera. Demostremos esto con una pequeña línea de preparación de texto que incluye:\n",
    "- Cargue el texto sin procesar.\n",
    "- Dividir en fichas.\n",
    "- Convertir a minúsculas.\n",
    "- Elimina la puntuación de cada ficha.\n",
    "- Filtre los tokens restantes que no sean alfabéticos.\n",
    "- Filtre los tokens que son palabras vacías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'whole', 'lower', 'arm', 'towards', 'viewer']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# load data\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar este ejemplo, podemos ver que, además de todas las demás transformaciones, palabras de parada como _a_ y _to_ han sido eliminadas. Observo que todavía nos quedan tokens como _nt_. \n",
    "\n",
    "Por desgracia, como vemos, siempre hay que seguir puliendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section17\"></a>\n",
    "# <font color=\"#004D7F\" size=5>1.7. Palabras raíz</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming se refiere al proceso de reducir cada palabra a su raíz o base. Por ejemplo, _fishing, fished, fisher_ todo se reduce a _fish_.\n",
    "\n",
    "Algunas aplicaciones, como la clasificación de documentos, pueden beneficiarse de la derivación para reducir el vocabulario y centrarse en el sentido o sentimiento de un documento en lugar de un significado más profundo. Hay muchos algoritmos de derivación, aunque un método popular y antiguo es el algoritmo de derivación de Porter. Este método está disponible en NLTK a través de la clase `PorterStemmer`. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', '.', 'he', 'lay', 'on', 'hi', 'armour-lik', 'back', ',', 'and', 'if', 'he', 'lift', 'hi', 'head', 'a', 'littl', 'he', 'could', 'see', 'hi', 'brown', 'belli', ',', 'slightli', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', '.', 'the', 'bed', 'wa', 'hardli', 'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', '.', 'hi', 'mani', 'leg', ',', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'wave', 'about', 'helplessli', 'as', 'he', 'look', '.', '``', 'what', \"'s\", 'happen', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# load data\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# stemming of words\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el ejemplo, puede ver que las palabras se han reducido a sus raíces, como problema se ha convertido\n",
    "en problema. También puede ver que la implementación de lematización también ha reducido los tokens a minúsculas,\n",
    "probablemente para búsquedas internas en tablas de palabras.\n",
    "\n",
    "Hay un buen conjunto de algoritmos de derivación y lematización para elegir en NLTK, si reducir las palabras a su raíz es algo que necesitas para tu proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6>2. Análisis de datos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section21\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.1. Conteo de palabras con `CountVectorizer`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` proporciona una forma sencilla de tokenizar una colección de documentos de texto y crear un vocabulario de palabras conocidas, pero también de codificar nuevos documentos utilizando ese vocabulario. Puedes usarlo de la siguiente manera:\n",
    "1. Cree una instancia de la clase `CountVectorizer`.\n",
    "2. Llame a la función `fit()` para aprender un vocabulario de uno o más documentos.\n",
    "3. Llame a la función `transform()` en uno o más documentos según sea necesario para codificar cada uno como un vector.\n",
    "\n",
    "Se devuelve un vector codificado con la longitud de todo el vocabulario y un número entero para el número de veces que apareció cada palabra en el documento. Debido a que estos vectores contendrán muchos ceros, los llamamos dispersos. Python proporciona una manera eficiente de manejar vectores dispersos en el paquete `scipy.sparse`. \n",
    "\n",
    "Los vectores devueltos por una llamada a `transform()` serán vectores dispersos, y puede volver a transformarlos en matrices NumPy para ver y comprender mejor lo que sucede llamando a la función `toarray()`. A continuación se muestra un ejemplo del uso de `CountVectorizer` para tokenizar, crear un vocabulario y luego codificar un documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre la clase [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arriba, puede ver que accedemos al vocabulario para ver qué se tokenizó exactamente al llamar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que todas las palabras se escribieron en minúsculas de forma predeterminada y que se ignoró la puntuación. Estos y otros aspectos de la tokenización se pueden configurar y lo animo a revisar todas las opciones en la documentación de la API. Ejecutar el ejemplo primero imprime el vocabulario, luego la forma del documento codificado.\n",
    "\n",
    "Podemos ver que hay 8 palabras en el vocabulario y, por lo tanto, los vectores codificados tienen una longitud de 8. Entonces podemos ver que el vector codificado es una matriz dispersa. Finalmente, podemos ver una versión de matriz del vector codificado que muestra un recuento de 1 ocurrencia para cada palabra excepto (_index_ e _id_ 7) que tiene una ocurrencia de 2.\n",
    "\n",
    "Es importante destacar que el mismo vectorizador se puede utilizar en documentos que contienen palabras no incluidas en el vocabulario. Estas palabras se ignoran y no se da ninguna cuenta en el vector resultante. Por ejemplo, a continuación se muestra un ejemplo del uso del vectorizador anterior para codificar un documento con una palabra en el vocabulario y una palabra que no lo es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode another document\n",
    "text2 = [\"the puppy\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar este ejemplo, se imprime la versión de matriz del vector disperso codificado que muestra una ocurrencia de una palabra en el vocabulario y la otra palabra que no está en el vocabulario se ignora por completo.\n",
    "\n",
    "Los vectores codificados luego se pueden usar directamente con un algoritmo de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section22\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.2. Frecuencias de palabras con `TfidfVectorizer`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los recuentos de palabras son un buen punto de partida, pero son muy básicos. Un problema con los recuentos simples es que algunas palabras como the aparecerán muchas veces y sus recuentos grandes no serán muy significativos en los vectores codificados. Una alternativa es calcular frecuencias de palabras y, con mucho, el método más popular se llama TF-IDF. Este es un acrónimo que significa _Term Frequency - Inverse Document Frequency_, que son los componentes de las puntuaciones resultantes asignadas a cada palabra.\n",
    "- __Frecuencia de términos__: Esto resume con qué frecuencia aparece una palabra dada dentro de un documento.\n",
    "- __Frecuencia de documento inversa__: esto reduce las palabras que aparecen mucho en los documentos.\n",
    "\n",
    "Sin entrar en matemáticas, TF-IDF son puntajes de frecuencia de palabras que intentan resaltar las palabras que son más interesantes, por ejemplo, frecuentes en un documento pero no entre documentos. El `TfidfVectorizer` tokenizará documentos, aprenderá el vocabulario y el documento inverso sobre ponderaciones de frecuencia y permitirle codificar nuevos documentos. Alternativamente, si ya tiene un `CountVectorizer` aprendido, puede usarlo con un `TfidfTransformer` para calcular las frecuencias inversas de los documentos y comenzar a codificar los documentos. \n",
    "\n",
    "Se utiliza el mismo proceso de creación, ajuste y transformación que con `CountVectorizer`. A continuación se muestra un ejemplo del uso de `TfidfVectorizer` para aprender vocabulario y frecuencias de documentos inversas en 3 documentos pequeños y luego codificar uno de esos documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre la clase [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\t\"The dog.\",\n",
    "\t\t\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aprende un vocabulario de 8 palabras de los documentos y a cada palabra se le asigna un índice entero único en el vector de salida. Las frecuencias inversas del documento se calculan para cada palabra del vocabulario, asignando la puntuación más baja de 1,0 a la palabra observada con mayor frecuencia: _the_ en el índice 7. \n",
    "\n",
    "Finalmente, el primer documento se codifica como una matriz dispersa de 8 elementos y podemos revisar el puntajes finales de cada palabra con diferentes valores para _the_, _fox_ y _dog_ de las otras palabras en el vocabulario.\n",
    "\n",
    "Las puntuaciones se normalizan a valores entre 0 y 1 y los vectores de documentos codificados pueden se pueden usar directamente con la mayoría de los algoritmos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section23\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.3. Hashing con `HashingVectorizer`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los conteos y las frecuencias pueden ser muy útiles, pero una limitación de estos métodos es que el vocabulario puede volverse muy extenso. Esto, a su vez, requerirá grandes vectores para codificar documentos e impondrá grandes requisitos en la memoria y ralentizará los algoritmos. Una solución inteligente es usar un hash de palabras unidireccional para convertirlas en números enteros. La parte inteligente es que no se requiere vocabulario y puede elegir un vector de longitud fija de longitud arbitraria. Un inconveniente es que el hash es una función unidireccional, por lo que no hay forma de volver a convertir la codificación en una palabra (que puede no importar para muchas tareas de aprendizaje supervisado).\n",
    "\n",
    "La clase `HashingVectorizer` implementa este enfoque que se puede usar para hash palabras, luego tokenice y codifique documentos según sea necesario. El siguiente ejemplo demuestra el `HashingVectorizer` para codificar un solo documento. Un tamaño de vector arbitrario de longitud fija de 20 fue elegido. Esto corresponde al rango de la función hash, donde los valores pequeños (como 20) puede dar lugar a colisiones de hash. Recordando las clases de Informática, creo hay heurísticas que puede usar para elegir la longitud del hash y la probabilidad de colisión según sobre el tamaño estimado del vocabulario (por ejemplo, un factor de carga del 75%). Ver cualquier buen libro de texto sobre el tema.\n",
    "\n",
    "Tenga en cuenta que este vectorizador no requiere una llamada para encajar en los documentos de datos de entrenamiento. En cambio, después de la creación de instancias, se puede usar directamente para comenzar a codificar documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre la clase [`HashingVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ejecución del ejemplo codifica el documento de muestra como una matriz dispersa de 20 elementos. Los valores del documento codificado corresponden a recuentos de palabras normalizados por defecto en el rango de -1 a 1, pero se pueden hacer recuentos de enteros simples cambiando la configuración predeterminada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6>3. Preparación de texto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No puede introducir texto sin procesar directamente en modelos de aprendizaje profundo. Los datos de texto deben codificarse como números para usarse como entrada o salida para el aprendizaje automático y los modelos de aprendizaje profundo, como las incrustaciones de palabras. \n",
    "\n",
    "La biblioteca de aprendizaje profundo de Keras proporciona algunas herramientas básicas para ayudarlo a preparar sus datos de texto. \n",
    "\n",
    "En este tutorial, descubrirá cómo puede usar Keras para preparar sus datos de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section31\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.1. Dividir palabras con `text_to_word_sequence`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un buen primer paso cuando se trabaja con texto es dividirlo en palabras. Las palabras se llaman __kens__ y el proceso de dividir el texto en tokens se denomina __tokenización__. Keras proporciona la función `text_to_word_sequence()` que puede usar para dividir el texto en una lista de palabras. Por defecto, esta función automáticamente hace 3 cosas:\n",
    "1. Divide palabras por espacio.\n",
    "2. Filtra la puntuación.\n",
    "3. Convierte texto a minúsculas (`lower=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre la clase [`text_to_word_sequence`](https://faroit.com/keras-docs/2.0.5/preprocessing/text/#text_to_word_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede cambiar cualquiera de estos valores predeterminados pasando argumentos a la función. A continuación se muestra un ejemplo del uso de la función `text_to_word_sequence()` para dividir un documento (en este caso, una cadena simple) en una lista de palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 12:12:34.243660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-05 12:12:40.915156: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/include:/usr/local/cuda-11.0/lib64:\n",
      "2022-12-05 12:12:40.915569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/include:/usr/local/cuda-11.0/lib64:\n",
      "2022-12-05 12:12:40.915602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section32\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.2. Codificación con `one_hot`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es popular representar un documento como una secuencia de valores enteros, donde cada palabra del documento se representa como un entero único. Keras proporciona la función `one_hot()` que puede usar para tokenizar y codificar un documento de texto en un solo paso. El nombre sugiere que creará una codificación en caliente del documento, lo cual no es el caso. \n",
    "\n",
    "En cambio, la función es un envoltorio para la función `hashing_trick()` descrita en la siguiente sección. La función devuelve una versión codificada en enteros del documento. El uso de una función hash significa que puede haber colisiones y no a todas las palabras se les asignarán valores enteros únicos. Al igual que con la función de `text_to_word_sequence()` en la sección anterior, la función `one_hot()` hará que el texto esté en minúsculas, filtrará la puntuación y dividirá las palabras en función de los espacios en blanco.\n",
    "\n",
    "Además del texto, se debe especificar el tamaño del vocabulario (palabras totales). Este podría ser el número total de palabras en el documento o más si tiene la intención de codificar documentos adicionales que contengan palabras adicionales. El tamaño del vocabulario define el espacio hash desde el cual se codifican las palabras. De forma predeterminada, se utiliza la función `hash`, aunque, como veremos en la siguiente sección, se pueden especificar funciones hash alternativas al llamar directamente a la función `hashing_trick()`.\n",
    "\n",
    "Podemos usar la función `text_to_word_sequence()` de la sección anterior para dividir el documento en palabras y luego usar un conjunto para representar solo las palabras únicas en el documento. El tamaño de este conjunto se puede utilizar para estimar el tamaño del vocabulario de un documento. Por ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre la clase [`one_hot`](https://faroit.com/keras-docs/2.0.5/preprocessing/text/#one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos juntar esto con la función `one_hot()` y codificar las palabras en el documento. El ejemplo completo se muestra a continuación. El tamaño del vocabulario se incrementa en un tercio para minimizar las colisiones al mezclar palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = one_hot(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el ejemplo, primero se imprime el tamaño del vocabulario como 8. Luego, el documento codificado se imprime como una matriz de palabras codificadas con números enteros.\n",
    "\n",
    "Nota: Dada la naturaleza estocástica de las redes neuronales, sus resultados específicos pueden variar. Considere ejecutar el ejemplo varias veces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section33\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.3. Codificación hash con `hashing_trick`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una limitación de las codificaciones de números enteros y base de conteo es que deben mantener un vocabulario de palabras y su asignación a números enteros. Una alternativa a este enfoque es utilizar una función hash unidireccional para convertir palabras en números enteros. Esto evita la necesidad de realizar un seguimiento de un vocabulario, que es más rápido y requiere menos memoria.\n",
    "\n",
    "Keras proporciona la función `hashing_trick()` que tokeniza y luego codifica el documento con enteros, al igual que la función `one_hot()`. Proporciona más flexibilidad, lo que le permite especificar la función hash como `hash` (la predeterminada) u otras funciones hash, como la función `md5` integrada o su propia función. A continuación se muestra un ejemplo de codificación de enteros de un documento utilizando la función hash md5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el ejemplo, se imprime el tamaño del vocabulario y el documento codificado en enteros. Podemos ver que el uso de una función hash diferente da como resultado enteros consistentes pero diferentes para palabras como la función `one_hot()` de la sección anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section34\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.4. API de `Tokenizador`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos analizado métodos prácticos únicos para preparar texto con Keras. Keras proporciona una API más sofisticada para preparar texto que se puede ajustar y reutilizar para preparar varios documentos de texto. Este puede ser el enfoque preferido para proyectos grandes. Keras proporciona la clase `Tokenizer` para preparar documentos de texto para el aprendizaje profundo. \n",
    "\n",
    "`Tokenizer` debe construirse y luego caber en documentos de texto sin formato o documentos de texto codificados con enteros. Por ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre la clase [`Tokenizer`](https://faroit.com/keras-docs/2.0.5/preprocessing/text/#tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ajustado, `Tokenizer` proporciona 4 atributos que puede usar para consultar lo que ha sido aprendido acerca de sus documentos:\n",
    "- __`word_counts`__: un mapeo de diccionario de palabras y sus recuentos de ocurrencia cuando se ajustó `Tokenizer`.\n",
    "- __`word_docs`__: un diccionario de mapeo de palabras y el número de documentos que aparecen.\n",
    "- __`word_index`__: un diccionario de palabras y sus números enteros asignados de forma única.\n",
    "- __`document_count`__: una asignación de diccionario y el número de documentos en los que aparecen calculados durante el ajuste.\n",
    "\n",
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que `Tokenizer` se ha ajustado a los datos de entrenamiento, se puede usar para codificar documentos en train o test. La función `texts_to_matrix()` en `Tokenizer` se puede usar para crear un vector por documento provisto por entrada. La longitud de los vectores es el tamaño total del vocabulario. Esta función proporciona un conjunto de esquemas de codificación de texto de modelo de bolsa de palabras estándar que se pueden proporcionar a través de un argumento `mode` para la función. Los modos disponibles incluyen:\n",
    "- __`binary`__: si cada palabra está presente o no en el documento. Este es el valor predeterminado.\n",
    "- __`count`__: el conteo de cada palabra en el documento.\n",
    "- __`tfidf`__: la puntuación de frecuencia de texto-frecuencia inversa del documento (TF-IDF) para cada palabra del documento.\n",
    "- __`freq`__: La frecuencia de cada palabra como proporción de palabras dentro de cada documento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar el ejemplo \n",
    "1. Ajusta `Tokenizer` con 5 documentos pequeños. \n",
    "2. Se imprimen los detalles de `Tokenizer` apto. \n",
    "3. Luego, los 5 documentos se codifican utilizando un conteo de palabras. \n",
    "4. Cada documento se codifica como un vector de 9 elementos con una posición para cada palabra y el valor del esquema de codificación elegido para cada posición de palabra. \n",
    "\n",
    "En este caso, se utiliza un modo de recuento de palabras simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='binary')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='tfidf')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='freq')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
